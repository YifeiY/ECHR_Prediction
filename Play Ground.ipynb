{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "# from doc_adapter import DocumentAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentAdapter():\n",
    "  def __init__(self,DATA_DIR = 'DS', FORMAT = '.pkl'):\n",
    "    self.DATA_DIR = DATA_DIR\n",
    "    self.FORMAT = FORMAT\n",
    "    self.shelves = dict()\n",
    "    self.exceptions = ['body',\"VIOLATED_ARTICLES\",\"VIOLATED_PARAGRAPHS\",\"VIOLATED_BULLETPOINTS\",\n",
    "                       \"NON_VIOLATED_ARTICLES\",\"NON_VIOLATED_PARAGRAPHS\",\"NON_VIOLATED_BULLETPOINTS\",\"CONCLUSION\"]\n",
    "    print('use .load_shelf(train_anon) to load anonymized training data. For other usages, see doc_adapter.py')\n",
    "    \n",
    "  \n",
    "  def make_shelf(self,partition,document_dicts,body = 'body',joining_literal = ' ',documents_name = None,remake = False):\n",
    "    '''document_dicts: documents stored as dictoinary elements\n",
    "    joining_literal: the char literal used to join the headings,\n",
    "    documents_name: the name of the document collection\n",
    "    remake: whether to remake the formated document storage'''\n",
    "    \n",
    "    # check if document archive already exists\n",
    "    \n",
    "    if not documents_name: documents_name = partition\n",
    "    file_name = self.DATA_DIR +'/' + documents_name + self.FORMAT\n",
    "    if remake and documets_name in os.listdir(self.DATA_DIR): os.remove(file_name)\n",
    "    if documents_name in os.listdir(self.DATA_DIR): shelf = np.load(file_name)\n",
    "    else:\n",
    "      assert type(document_dicts) == dict\n",
    "      shelf = dict() # shelf is a collection of document archives\n",
    "      for k,document_dict in document_dicts.items():\n",
    "        shelf[k] = dict()\n",
    "        if 'body' not in document_dict.keys(): print('document',k,'is missing its body, or its body is specified with a different key than \\'body\\'')\n",
    "        temp = joining_literal.join([str(_v) for (_k,_v) in document_dict.items() if _k not in self.exceptions])\n",
    "        shelf[k]['headings'] = temp\n",
    "        shelf[k]['body'] = document_dict['body']\n",
    "        for e in self.exceptions:\n",
    "          shelf[k][e] = document_dict[e] # preserve the targets\n",
    "    document_dicts = None\n",
    "    pickle.dump(shelf,open(self.DATA_DIR + '/' + documents_name + self.FORMAT,'wb'))\n",
    "    if partition: self.shelves[partition] = shelf\n",
    "    print('loaded',documents_name,'to shelf partition',partition,'\\tCurrent partitions:',','.join(str(k) for k in self.shelves.keys()))\n",
    "  \n",
    "  \n",
    "  def load_shelf(self,shelf,partition =None):\n",
    "    if not partition: partition = str(shelf)\n",
    "    shelf = self.DATA_DIR +'/' + shelf + self.FORMAT\n",
    "    self.shelves[partition] = pickle.load(open(shelf,'rb'))\n",
    "    print(partition,'shelf loaded')\n",
    "    \n",
    "  \n",
    "  def make_shelf_from_raw(self,ECHR_RAW_DIR = '/ECHR_Dataset'):\n",
    "    '''the directory should contain i folders, for example, [train,test,dev],\n",
    "    within each folder, there should be a set of JSON files.\n",
    "    A copy of the data can be downloaded at https://archive.org/download/ECHR-ACL2019/ECHR_Dataset.zip'''\n",
    "\n",
    "    \n",
    "    ECHR_RAW_DIR = self.DATA_DIR + ECHR_RAW_DIR\n",
    "    partitions = {'train':'EN_train','test':'EN_test','dev':'EN_dev','train_anon':'EN_train_Anon','test_anon':'EN_test_Anon','dev_anon':'EN_dev_Anon',}\n",
    "    \n",
    "    def load_files(directory):\n",
    "        raw_shelf = dict()\n",
    "        for file in [ECHR_RAW_DIR + '/' + directory + '/' + file_name for file_name in os.listdir(ECHR_RAW_DIR +'/' + directory)]:\n",
    "          temp = json.load(open(file))\n",
    "          temp['body'] = temp['TEXT']\n",
    "          del temp['TEXT']\n",
    "          raw_shelf[file.split('/')[-1]] = temp\n",
    "        return raw_shelf\n",
    "        \n",
    "    for (partition,directory) in partitions.items():\n",
    "      self.make_shelf(partition,load_files(directory))\n",
    "          \n",
    "        \n",
    "    \n",
    "  def make_corpus(self):\n",
    "    corpus_filename = self.DATA_DIR + '/corpus.txt'\n",
    "    if 'corpus.txt' in os.listdir(self.DATA_DIR): os.remove(corpus_filename)\n",
    "    with open(corpus_filename,'w',encoding='utf8') as corpus_file:\n",
    "      for partition,shelf in self.shelves.items():\n",
    "        print('making corpus for',partition,'partition')\n",
    "        for doc in shelf.values():\n",
    "          corpus_file.write(doc['headings']  + '\\n'+'\\n'.join(doc['body']) + \"\\n\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "  def remake(self):\n",
    "    print('remaking datasets and corpus file from raw data')\n",
    "    self.make_shelf_from_raw()\n",
    "    self.make_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use .load_shelf(train_anon) to load anonymized training data. For other usages, see doc_adapter.py\n"
     ]
    }
   ],
   "source": [
    "da = DocumentAdapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaking datasets and corpus file from raw data\n",
      "loaded train to shelf partition train \tCurrent partitions: train\n",
      "loaded test to shelf partition test \tCurrent partitions: train,test\n",
      "loaded dev to shelf partition dev \tCurrent partitions: train,test,dev\n",
      "loaded train_anon to shelf partition train_anon \tCurrent partitions: train,test,dev,train_anon\n",
      "loaded test_anon to shelf partition test_anon \tCurrent partitions: train,test,dev,train_anon,test_anon\n",
      "loaded dev_anon to shelf partition dev_anon \tCurrent partitions: train,test,dev,train_anon,test_anon,dev_anon\n",
      "making corpus for train partition\n",
      "making corpus for test partition\n",
      "making corpus for dev partition\n",
      "making corpus for train_anon partition\n",
      "making corpus for test_anon partition\n",
      "making corpus for dev_anon partition\n"
     ]
    }
   ],
   "source": [
    "da.remake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file', 'no_cleaning_data.pkl', 'no_cleaning_onefifth.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-b2b57bc46c91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m\"pp\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DS/ECHR_Dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
