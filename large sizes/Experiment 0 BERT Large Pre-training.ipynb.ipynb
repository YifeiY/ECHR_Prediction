{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bam/Desktop/ECHR Publication Experiments'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir(\"/tmp/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COURPUS_DIR = '/home/bam/Desktop/ECHR Publication Experiments/DS/corpus.txt'\n",
    "BERT_BASE_DIR ='/home/bam/Desktop/ECHR Publication Experiments/bert_master'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0618 00:24:19.770558 140488205842240 module_wrapper.py:139] From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "W0618 00:24:19.770655 140488205842240 module_wrapper.py:139] From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bam/Desktop/ECHR Publication Experiments/bert_master/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0618 00:24:19.770857 140488205842240 module_wrapper.py:139] From /home/bam/Desktop/ECHR Publication Experiments/bert_master/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
      "\n",
      "W0618 00:24:19.824885 140488205842240 module_wrapper.py:139] From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W0618 00:24:19.825637 140488205842240 module_wrapper.py:139] From /home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:*** Reading from input files ***\n",
      "I0618 00:24:19.825723 140488205842240 create_pretraining_data.py:446] *** Reading from input files ***\n",
      "INFO:tensorflow:  /home/bam/Desktop/ECHR Publication Experiments/DS/corpus.txt\n",
      "I0618 00:24:19.825767 140488205842240 create_pretraining_data.py:448]   /home/bam/Desktop/ECHR Publication Experiments/DS/corpus.txt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py\", line 469, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/home/bam/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/bam/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/bam/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py\", line 454, in main\n",
      "    rng)\n",
      "  File \"/home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py\", line 217, in create_training_instances\n",
      "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
      "  File \"/home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py\", line 323, in create_instances_from_document\n",
      "    tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
      "  File \"/home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py\", line 359, in create_masked_lm_predictions\n",
      "    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and\n",
      "  File \"/home/bam/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/flags.py\", line 77, in __getattribute__\n",
      "    return self.__dict__['__wrapped'].__getattribute__(name)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# !cd \"/home/bam/Desktop/ECHR Publication Experiments\"\\\n",
    "!python \"/home/bam/Desktop/ECHR Publication Experiments/bert_master/create_pretraining_data.py\" \\\n",
    "  --input_file=\"/home/bam/Desktop/ECHR Publication Experiments/DS/corpus.txt\" \\\n",
    "  --output_file=\"/home/bam/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-24_H-1024_A-16/further/tf_examples.tfrecord\" \\\n",
    "  --vocab_file=\"/home/bam/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-24_H-1024_A-16/vocab.txt\"\\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=512 \\\n",
    "  --max_predictions_per_seq=77 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --random_seed=12345 \\\n",
    "  --dupe_factor=10 \\\n",
    "  --do_whole_word_mask=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-ac4fe0028856>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-ac4fe0028856>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python run_pretraining.py --input_file=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/tf_examples.tfrecord\" --output_dir=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/pretraining_output\" --do_train=True --do_eval=True --bert_config_file=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/bert_config.json\" --init_checkpoint=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/bert_model.ckpt\" --train_batch_size=6 --max_seq_length=512 --max_predictions_per_seq=77 --num_train_steps=10000 --num_warmup_steps=1000 --learning_rate=2e-5\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python run_pretraining.py --input_file=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/tf_examples.tfrecord\" --output_dir=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/pretraining_output\" --do_train=True --do_eval=True --bert_config_file=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/bert_config.json\" --init_checkpoint=\"C:/Users/Yifei/Desktop/ECHR Publication Experiments/bert_master/pre-trained/uncased_L-12_H-768_A-12/bert_model.ckpt\" --train_batch_size=6 --max_seq_length=512 --max_predictions_per_seq=77 --num_train_steps=10000 --num_warmup_steps=1000 --learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
